{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import jieba\n",
    "import logging\n",
    "import codecs\n",
    "import traceback\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from collections import Counter\n",
    "from sklearn import metrics\n",
    "import parameter as pa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCluster(object):\n",
    "    # 初始化函数,重写父类函数\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def seg_words(self, sentence):\n",
    "        seg_list = jieba.cut(sentence)  # 默认是精确模式\n",
    "        return \" \".join(seg_list)       # 分词，然后将结果列表形式转换为字符串\n",
    "\n",
    "    # 加载用户词典\n",
    "    def load_userdictfile(self, dict_file):\n",
    "        jieba.load_userdict(dict_file)\n",
    "\n",
    "    def load_processfile(self, process_file):\n",
    "        corpus_list = []\n",
    "        try:\n",
    "            fp = open(process_file, \"r\")\n",
    "            for line in fp:\n",
    "                conline = line.strip()\n",
    "                corpus_list.append(conline)\n",
    "            return True, corpus_list\n",
    "        except:\n",
    "            logging.error(traceback.format_exc())\n",
    "            return False, \"get process file fail\"\n",
    "\n",
    "    def output_file(self, out_file, item):\n",
    "\n",
    "        try:\n",
    "            fw = open(out_file, \"a\")\n",
    "            fw.write('%s' % (item.encode(\"utf-8\")))\n",
    "            fw.close()\n",
    "        except:\n",
    "            logging.error(traceback.format_exc())\n",
    "            return False, \"out file fail\"\n",
    "\n",
    "    # 释放内存资源\n",
    "    def __del__(self):\n",
    "        pass\n",
    "\n",
    "    def process(self, process_file, tf_ResFileName, tfidf_ResFileName, num_clusters, cluster_ResFileName):\n",
    "        try:\n",
    "            sen_seg_list = []\n",
    "            flag, lines = self.load_processfile(process_file)\n",
    "            if flag == False:\n",
    "                logging.error(\"load error\")\n",
    "                return False, \"load error\"\n",
    "            for line in lines:\n",
    "                for k in pa.l:\n",
    "                    line = line.replace(k, '')\n",
    "\n",
    "                sen_seg_list.append(self.seg_words(line))\n",
    "\n",
    "            # 该类会将文本中的词语转换为词频矩阵，矩阵元素a[i][j] 表示j词在i类文本下的词频\n",
    "            tf_vectorizer = CountVectorizer(min_df=0.01)\n",
    "\n",
    "            # fit_transform是将文本转为词频矩阵\n",
    "            tf_matrix = tf_vectorizer.fit_transform(sen_seg_list)\n",
    " \n",
    "            # 聚类分析\n",
    "            clusterRes = codecs.open(cluster_ResFileName, 'w', 'utf-8')\n",
    "\n",
    "            km = KMeans(n_clusters=num_clusters)\n",
    "            km.fit(tf_matrix)\n",
    "            order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "\n",
    "            for i in range(num_clusters):\n",
    "                print(\"Cluster %d words:\" % i, end='')\n",
    "\n",
    "                for ind in order_centroids[i, :6]:  # 每个聚类选 6 个词\n",
    "                    print(ind)\n",
    "                    # print(' %s' % tf_matrix.ix[tf_matrix[ind].split(' ')].values.tolist()[0][0].encode('utf-8', 'ignore'),\n",
    "                    #       end=',')\n",
    "\n",
    "\n",
    "            print('or : %s'%order_centroids)\n",
    "            print (metrics.silhouette_score(tf_matrix, km.labels_, metric='euclidean'))\n",
    "            print (Counter(km.labels_))  # 打印每个类多少人\n",
    "            # 中心点\n",
    "\n",
    "            # 每个样本所属的簇\n",
    "            print(km.inertia_)\n",
    "            count = 1\n",
    "            while count <= len(km.labels_):\n",
    "                clusterRes.write(str(count) + '\\t' + str(km.labels_[count - 1]))\n",
    "                clusterRes.write('\\r\\n')\n",
    "                count = count + 1\n",
    "            clusterRes.close()\n",
    "\n",
    "\n",
    "\n",
    "        except:\n",
    "            logging.error(traceback.format_exc())\n",
    "            return False, \"process fail\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "    tc = TextCluster()\n",
    "    tc.process(\"data2.txt\", \"tf_Result.txt\", \"tfidf_Result.txt\", 2, \"cluster_Result.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
