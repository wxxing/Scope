{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用卷积神经网络进行公司经营范围分类\n",
    "\n",
    "    Python 3\n",
    "    TensorFlow 1.3以上\n",
    "    numpy\n",
    "    scikit-learn\n",
    "    scipy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本次训练使用了其中的8个分类。\n",
    "\n",
    "类别如下：\n",
    "\n",
    "'服装', '工业', '农业', '运输', '信息', '食品', '建筑', '金融'\n",
    "\n",
    "数据集划分如下：\n",
    "\n",
    "\n",
    "\n",
    "    cnews.train.txt: 训练集(38988条)\n",
    "    cnews.val.txt: 验证集(4155条)\n",
    "    cnews.test.txt: 测试集(8835条)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn import metrics\n",
    "\n",
    "from cnews_loader import read_vocab, read_category, batch_iter, process_file, build_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data/cnews_loader.py为数据的预处理文件。\n",
    "\n",
    "    read_file(): 读取文件数据;\n",
    "    build_vocab(): 构建词汇表，使用字符级的表示，这一函数会将词汇表存储下来，避免每一次重复处理;\n",
    "    read_vocab(): 读取上一步存储的词汇表，转换为{词：id}表示;\n",
    "    read_category(): 将分类目录固定，转换为{类别: id}表示;\n",
    "    to_words(): 将一条由id表示的数据重新转换为文字;\n",
    "    process_file(): 将数据集从文字转换为固定长度的id序列表示;\n",
    "    batch_iter(): 为神经网络的训练准备经过shuffle的批次的数据。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TCNNConfig(object):\n",
    "    \"\"\"CNN配置参数\"\"\"\n",
    "\n",
    "    embedding_dim = 64  # 词向量维度\n",
    "    seq_length = 600  # 序列长度\n",
    "    num_classes = 8  # 类别数\n",
    "    num_filters = 256  # 卷积核数目\n",
    "    kernel_size = 5  # 卷积核尺寸\n",
    "    vocab_size = 5000  # 词汇表达小\n",
    "\n",
    "    hidden_dim = 128  # 全连接层神经元\n",
    "\n",
    "    dropout_keep_prob = 0.5  # dropout保留比例\n",
    "    learning_rate = 1e-3  # 学习率\n",
    "\n",
    "    batch_size = 64  # 每批训练大小\n",
    "    num_epochs = 10  # 总迭代轮次\n",
    "\n",
    "    print_per_batch = 100  # 每多少轮输出一次结果\n",
    "    save_per_batch = 10  # 每多少轮存入tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(object):\n",
    "    \"\"\"文本分类，CNN模型\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "\n",
    "        # 三个待输入的数据\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, self.config.seq_length], name='input_x')\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, self.config.num_classes], name='input_y')\n",
    "        self.keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "        self.cnn()\n",
    "\n",
    "    def cnn(self):\n",
    "        \"\"\"CNN模型\"\"\"\n",
    "        # 词向量映射\n",
    "        with tf.device('/cpu:0'):\n",
    "            embedding = tf.get_variable('embedding', [self.config.vocab_size, self.config.embedding_dim])\n",
    "            embedding_inputs = tf.nn.embedding_lookup(embedding, self.input_x)\n",
    "\n",
    "        with tf.name_scope(\"cnn\"):\n",
    "            # CNN layer\n",
    "            conv = tf.layers.conv1d(embedding_inputs, self.config.num_filters, self.config.kernel_size, name='conv')\n",
    "            # global max pooling layer\n",
    "            gmp = tf.reduce_max(conv, reduction_indices=[1], name='gmp')\n",
    "\n",
    "        with tf.name_scope(\"score\"):\n",
    "            # 全连接层，后面接dropout以及relu激活\n",
    "            fc = tf.layers.dense(gmp, self.config.hidden_dim, name='fc1')\n",
    "            fc = tf.contrib.layers.dropout(fc, self.keep_prob)\n",
    "            fc = tf.nn.relu(fc)\n",
    "\n",
    "            # 分类器\n",
    "            self.logits = tf.layers.dense(fc, self.config.num_classes, name='fc2')\n",
    "            self.y_pred_cls = tf.argmax(tf.nn.softmax(self.logits), 1)  # 预测类别\n",
    "\n",
    "        with tf.name_scope(\"optimize\"):\n",
    "            # 损失函数，交叉熵\n",
    "            cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=self.logits, labels=self.input_y)\n",
    "            self.loss = tf.reduce_mean(cross_entropy)\n",
    "            # 优化器\n",
    "            self.optim = tf.train.AdamOptimizer(learning_rate=self.config.learning_rate).minimize(self.loss)\n",
    "\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            # 准确率\n",
    "            correct_pred = tf.equal(tf.argmax(self.input_y, 1), self.y_pred_cls)\n",
    "            self.acc = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '/home/wu/文档/modeling/Scope_data/CNN_RNN/data/cnews/'\n",
    "train_dir = os.path.join(base_dir, 'cnews.train.txt')\n",
    "test_dir = os.path.join(base_dir, 'cnews.test.txt')\n",
    "val_dir = os.path.join(base_dir, 'cnews.val.txt')\n",
    "vocab_dir = os.path.join(base_dir, 'cnews.vocab.txt')\n",
    "\n",
    "save_dir = '/home/wu/文档/modeling/Scope_data/CNN_RNN/checkpoints/textcnn/'\n",
    "save_path = os.path.join(save_dir, 'best_validation')  # 最佳验证结果保存路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_dif(start_time):\n",
    "    \"\"\"获取已使用时间\"\"\"\n",
    "    end_time = time.time()\n",
    "    time_dif = end_time - start_time\n",
    "    return timedelta(seconds=int(round(time_dif)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_data(x_batch, y_batch, keep_prob):\n",
    "    feed_dict = {\n",
    "        model.input_x: x_batch,\n",
    "        model.input_y: y_batch,\n",
    "        model.keep_prob: keep_prob\n",
    "    }\n",
    "    return feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sess, x_, y_):\n",
    "    \"\"\"评估在某一数据上的准确率和损失\"\"\"\n",
    "    data_len = len(x_)\n",
    "    batch_eval = batch_iter(x_, y_, 128)\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "    for x_batch, y_batch in batch_eval:\n",
    "        batch_len = len(x_batch)\n",
    "        feed_dict = feed_data(x_batch, y_batch, 1.0)\n",
    "        loss, acc = sess.run([model.loss, model.acc], feed_dict=feed_dict)\n",
    "        total_loss += loss * batch_len\n",
    "        total_acc += acc * batch_len\n",
    "\n",
    "    return total_loss / data_len, total_acc / data_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    print(\"Configuring TensorBoard and Saver...\")\n",
    "    # 配置 Tensorboard，重新训练时，请将tensorboard文件夹删除，不然图会覆盖\n",
    "    tensorboard_dir = '/home/wu/文档/modeling/Scope_data/CNN_RNN/tensorboard/textcnn'\n",
    "    if not os.path.exists(tensorboard_dir):\n",
    "        os.makedirs(tensorboard_dir)\n",
    "\n",
    "    tf.summary.scalar(\"loss\", model.loss)\n",
    "    tf.summary.scalar(\"accuracy\", model.acc)\n",
    "    merged_summary = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter(tensorboard_dir)\n",
    "\n",
    "    # 配置 Saver\n",
    "    saver = tf.train.Saver()\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    print(\"Loading training and validation data...\")\n",
    "    # 载入训练集与验证集\n",
    "    start_time = time.time()\n",
    "    x_train, y_train = process_file(train_dir, word_to_id, cat_to_id, config.seq_length)\n",
    "    x_val, y_val = process_file(val_dir, word_to_id, cat_to_id, config.seq_length)\n",
    "    time_dif = get_time_dif(start_time)\n",
    "    print(\"Time usage:\", time_dif)\n",
    "\n",
    "    # 创建session\n",
    "    session = tf.Session()\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    writer.add_graph(session.graph)\n",
    "\n",
    "    print('Training and evaluating...')\n",
    "    start_time = time.time()\n",
    "    total_batch = 0  # 总批次\n",
    "    best_acc_val = 0.0  # 最佳验证集准确率\n",
    "    last_improved = 0  # 记录上一次提升批次\n",
    "    require_improvement = 1000  # 如果超过1000轮未提升，提前结束训练\n",
    "    train_scores = []\n",
    "    test_scores = []\n",
    "\n",
    "    flag = False\n",
    "    for epoch in range(config.num_epochs):\n",
    "        print('Epoch:', epoch + 1)\n",
    "        batch_train = batch_iter(x_train, y_train, config.batch_size)\n",
    "        for x_batch, y_batch in batch_train:\n",
    "            feed_dict = feed_data(x_batch, y_batch, config.dropout_keep_prob)\n",
    "\n",
    "            if total_batch % config.save_per_batch == 0:\n",
    "                # 每多少轮次将训练结果写入tensorboard scalar\n",
    "                s = session.run(merged_summary, feed_dict=feed_dict)\n",
    "                writer.add_summary(s, total_batch)\n",
    "\n",
    "            if total_batch % config.print_per_batch == 0:\n",
    "                # 每多少轮次输出在训练集和验证集上的性能\n",
    "                feed_dict[model.keep_prob] = 1.0\n",
    "                loss_train, acc_train = session.run([model.loss, model.acc], feed_dict=feed_dict)\n",
    "                loss_val, acc_val = evaluate(session, x_val, y_val)  # todo\n",
    "\n",
    "                if acc_val > best_acc_val:\n",
    "                    # 保存最好结果\n",
    "                    best_acc_val = acc_val\n",
    "                    last_improved = total_batch\n",
    "                    saver.save(sess=session, save_path=save_path)\n",
    "                    improved_str = '*'\n",
    "                else:\n",
    "                    improved_str = ''\n",
    "\n",
    "                time_dif = get_time_dif(start_time)\n",
    "                msg = 'Iter: {0:>6}, Train Loss: {1:>6.2}, Train Acc: {2:>7.2%},' \\\n",
    "                      + ' Val Loss: {3:>6.2}, Val Acc: {4:>7.2%}, Time: {5} {6}'\n",
    "                print(msg.format(total_batch, loss_train, acc_train, loss_val, acc_val, time_dif, improved_str))\n",
    "\n",
    "            session.run(model.optim, feed_dict=feed_dict)  # 运行优化\n",
    "            total_batch += 1\n",
    "\n",
    "            if total_batch - last_improved > require_improvement:\n",
    "                # 验证集正确率长期不提升，提前结束训练\n",
    "                print(\"No optimization for a long time, auto-stopping...\")\n",
    "                flag = True\n",
    "                break  # 跳出循环\n",
    "        if flag:  # 同上\n",
    "            break\n",
    "    return train_scores,test_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    print(\"Loading test data...\")\n",
    "    start_time = time.time()\n",
    "    x_test, y_test = process_file(test_dir, word_to_id, cat_to_id, config.seq_length)\n",
    "\n",
    "    session = tf.Session()\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess=session, save_path=save_path)  # 读取保存的模型\n",
    "\n",
    "    print('Testing...')\n",
    "    loss_test, acc_test = evaluate(session, x_test, y_test)\n",
    "    msg = 'Test Loss: {0:>6.2}, Test Acc: {1:>7.2%}'\n",
    "    print(msg.format(loss_test, acc_test))\n",
    "\n",
    "    batch_size = 128\n",
    "    data_len = len(x_test)\n",
    "    num_batch = int((data_len - 1) / batch_size) + 1\n",
    "\n",
    "    y_test_cls = np.argmax(y_test, 1)\n",
    "    y_pred_cls = np.zeros(shape=len(x_test), dtype=np.int32)  # 保存预测结果\n",
    "    for i in range(num_batch):  # 逐批次处理\n",
    "        start_id = i * batch_size\n",
    "        end_id = min((i + 1) * batch_size, data_len)\n",
    "        feed_dict = {\n",
    "            model.input_x: x_test[start_id:end_id],\n",
    "            model.keep_prob: 1.0\n",
    "        }\n",
    "        y_pred_cls[start_id:end_id] = session.run(model.y_pred_cls, feed_dict=feed_dict)\n",
    "\n",
    "    # 评估\n",
    "    print(\"Precision, Recall and F1-Score...\")\n",
    "    print(metrics.classification_report(y_test_cls, y_pred_cls, target_names=categories))\n",
    "\n",
    "    # 混淆矩阵\n",
    "    print(\"Confusion Matrix...\")\n",
    "    cm = metrics.confusion_matrix(y_test_cls, y_pred_cls)\n",
    "    print(cm)\n",
    "\n",
    "    time_dif = get_time_dif(start_time)\n",
    "    print(\"Time usage:\", time_dif)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = TCNNConfig()\n",
    "if not os.path.exists(vocab_dir):  # 如果不存在词汇表，重建\n",
    "    build_vocab(train_dir, vocab_dir, config.vocab_size)\n",
    "categories, cat_to_id = read_category()\n",
    "words, word_to_id = read_vocab(vocab_dir)\n",
    "config.vocab_size = len(words)\n",
    "model = TextCNN(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuring TensorBoard and Saver...\n",
      "Loading training and validation data...\n",
      "Time usage: 0:00:03\n",
      "Training and evaluating...\n",
      "Epoch: 1\n",
      "Iter:      0, Train Loss:    2.1, Train Acc:   4.69%, Val Loss:    2.1, Val Acc:   2.46%, Time: 0:00:13 *\n",
      "Iter:    100, Train Loss:   0.61, Train Acc:  79.69%, Val Loss:   0.64, Val Acc:  80.30%, Time: 0:02:29 *\n",
      "Iter:    200, Train Loss:   0.71, Train Acc:  75.00%, Val Loss:   0.49, Val Acc:  84.39%, Time: 0:04:44 *\n",
      "Iter:    300, Train Loss:   0.41, Train Acc:  85.94%, Val Loss:   0.44, Val Acc:  86.78%, Time: 0:07:00 *\n",
      "Iter:    400, Train Loss:   0.25, Train Acc:  93.75%, Val Loss:   0.34, Val Acc:  90.13%, Time: 0:09:16 *\n",
      "Iter:    500, Train Loss:   0.31, Train Acc:  87.50%, Val Loss:   0.31, Val Acc:  90.73%, Time: 0:11:32 *\n",
      "Iter:    600, Train Loss:   0.23, Train Acc:  93.75%, Val Loss:   0.29, Val Acc:  91.35%, Time: 0:13:47 *\n",
      "Epoch: 2\n",
      "Iter:    700, Train Loss:   0.36, Train Acc:  90.62%, Val Loss:   0.28, Val Acc:  91.59%, Time: 0:16:03 *\n",
      "Iter:    800, Train Loss:   0.12, Train Acc:  93.75%, Val Loss:   0.27, Val Acc:  91.74%, Time: 0:18:19 *\n",
      "Iter:    900, Train Loss:   0.11, Train Acc:  95.31%, Val Loss:   0.27, Val Acc:  92.10%, Time: 0:19:55 *\n",
      "Iter:   1000, Train Loss:   0.36, Train Acc:  84.38%, Val Loss:   0.26, Val Acc:  92.27%, Time: 0:21:15 *\n",
      "Iter:   1100, Train Loss:   0.32, Train Acc:  92.19%, Val Loss:   0.25, Val Acc:  92.70%, Time: 0:22:37 *\n",
      "Iter:   1200, Train Loss:   0.25, Train Acc:  92.19%, Val Loss:   0.25, Val Acc:  92.82%, Time: 0:23:58 *\n",
      "Epoch: 3\n",
      "Iter:   1300, Train Loss:   0.07, Train Acc:  98.44%, Val Loss:   0.25, Val Acc:  92.77%, Time: 0:25:26 \n",
      "Iter:   1400, Train Loss:  0.053, Train Acc:  98.44%, Val Loss:   0.25, Val Acc:  92.99%, Time: 0:26:47 *\n",
      "Iter:   1500, Train Loss:   0.18, Train Acc:  93.75%, Val Loss:   0.24, Val Acc:  92.77%, Time: 0:28:04 \n",
      "Iter:   1600, Train Loss:    0.3, Train Acc:  90.62%, Val Loss:   0.25, Val Acc:  92.89%, Time: 0:29:19 \n",
      "Iter:   1700, Train Loss:  0.085, Train Acc:  98.44%, Val Loss:   0.24, Val Acc:  92.92%, Time: 0:30:39 \n",
      "Iter:   1800, Train Loss:  0.062, Train Acc:  98.44%, Val Loss:   0.24, Val Acc:  92.92%, Time: 0:31:58 \n",
      "Epoch: 4\n",
      "Iter:   1900, Train Loss:  0.073, Train Acc:  98.44%, Val Loss:   0.24, Val Acc:  93.16%, Time: 0:33:15 *\n",
      "Iter:   2000, Train Loss:   0.24, Train Acc:  92.19%, Val Loss:   0.25, Val Acc:  93.02%, Time: 0:34:36 \n",
      "Iter:   2100, Train Loss:  0.059, Train Acc: 100.00%, Val Loss:   0.26, Val Acc:  93.14%, Time: 0:35:56 \n",
      "Iter:   2200, Train Loss:   0.14, Train Acc:  95.31%, Val Loss:   0.25, Val Acc:  92.63%, Time: 0:37:16 \n",
      "Iter:   2300, Train Loss:   0.15, Train Acc:  92.19%, Val Loss:   0.23, Val Acc:  93.06%, Time: 0:38:36 \n",
      "Iter:   2400, Train Loss:   0.13, Train Acc:  96.88%, Val Loss:   0.25, Val Acc:  93.14%, Time: 0:40:02 \n",
      "Epoch: 5\n",
      "Iter:   2500, Train Loss:  0.029, Train Acc:  98.44%, Val Loss:   0.24, Val Acc:  93.16%, Time: 0:41:20 *\n",
      "Iter:   2600, Train Loss:  0.091, Train Acc:  98.44%, Val Loss:   0.25, Val Acc:  92.56%, Time: 0:42:43 \n",
      "Iter:   2700, Train Loss:   0.12, Train Acc:  95.31%, Val Loss:   0.24, Val Acc:  93.16%, Time: 0:44:09 *\n",
      "Iter:   2800, Train Loss:   0.17, Train Acc:  95.31%, Val Loss:   0.25, Val Acc:  92.94%, Time: 0:45:31 \n",
      "Iter:   2900, Train Loss:    0.1, Train Acc:  96.88%, Val Loss:   0.25, Val Acc:  92.68%, Time: 0:46:53 \n",
      "Iter:   3000, Train Loss:   0.23, Train Acc:  90.62%, Val Loss:   0.23, Val Acc:  93.35%, Time: 0:48:17 *\n",
      "Epoch: 6\n",
      "Iter:   3100, Train Loss:   0.05, Train Acc:  96.88%, Val Loss:   0.23, Val Acc:  93.35%, Time: 0:49:40 *\n",
      "Iter:   3200, Train Loss:   0.06, Train Acc:  98.44%, Val Loss:   0.25, Val Acc:  93.47%, Time: 0:51:03 *\n",
      "Iter:   3300, Train Loss:  0.059, Train Acc:  98.44%, Val Loss:   0.26, Val Acc:  93.14%, Time: 0:52:26 \n",
      "Iter:   3400, Train Loss:  0.082, Train Acc:  95.31%, Val Loss:   0.24, Val Acc:  93.71%, Time: 0:53:49 *\n",
      "Iter:   3500, Train Loss:   0.08, Train Acc:  98.44%, Val Loss:   0.25, Val Acc:  93.30%, Time: 0:55:13 \n",
      "Iter:   3600, Train Loss:   0.12, Train Acc:  93.75%, Val Loss:   0.25, Val Acc:  92.92%, Time: 0:56:38 \n",
      "Epoch: 7\n",
      "Iter:   3700, Train Loss:  0.061, Train Acc:  98.44%, Val Loss:   0.27, Val Acc:  93.47%, Time: 0:58:02 \n",
      "Iter:   3800, Train Loss:  0.041, Train Acc:  98.44%, Val Loss:   0.27, Val Acc:  93.16%, Time: 0:59:23 \n",
      "Iter:   3900, Train Loss:  0.056, Train Acc:  98.44%, Val Loss:   0.27, Val Acc:  93.33%, Time: 1:00:48 \n",
      "Iter:   4000, Train Loss:  0.034, Train Acc: 100.00%, Val Loss:   0.25, Val Acc:  93.38%, Time: 1:02:13 \n",
      "Iter:   4100, Train Loss:  0.052, Train Acc:  98.44%, Val Loss:   0.27, Val Acc:  93.50%, Time: 1:03:39 \n",
      "Iter:   4200, Train Loss:  0.088, Train Acc:  96.88%, Val Loss:   0.27, Val Acc:  93.38%, Time: 1:05:04 \n",
      "Epoch: 8\n",
      "Iter:   4300, Train Loss:  0.098, Train Acc:  98.44%, Val Loss:   0.26, Val Acc:  93.69%, Time: 1:06:28 \n",
      "Iter:   4400, Train Loss:   0.07, Train Acc:  95.31%, Val Loss:   0.27, Val Acc:  93.79%, Time: 1:07:53 *\n",
      "Iter:   4500, Train Loss:  0.014, Train Acc: 100.00%, Val Loss:   0.29, Val Acc:  93.04%, Time: 1:09:18 \n",
      "Iter:   4600, Train Loss:  0.029, Train Acc: 100.00%, Val Loss:   0.28, Val Acc:  93.59%, Time: 1:10:43 \n",
      "Iter:   4700, Train Loss:  0.084, Train Acc:  98.44%, Val Loss:   0.27, Val Acc:  93.09%, Time: 1:12:07 \n",
      "Iter:   4800, Train Loss:   0.12, Train Acc:  95.31%, Val Loss:   0.27, Val Acc:  93.42%, Time: 1:13:26 \n",
      "Epoch: 9\n",
      "Iter:   4900, Train Loss:  0.066, Train Acc:  95.31%, Val Loss:    0.3, Val Acc:  93.23%, Time: 1:14:43 \n",
      "Iter:   5000, Train Loss:   0.12, Train Acc:  95.31%, Val Loss:    0.3, Val Acc:  93.11%, Time: 1:16:04 \n",
      "Iter:   5100, Train Loss:  0.028, Train Acc:  98.44%, Val Loss:   0.29, Val Acc:  93.35%, Time: 1:17:25 \n",
      "Iter:   5200, Train Loss:  0.071, Train Acc:  98.44%, Val Loss:   0.29, Val Acc:  93.40%, Time: 1:18:51 \n",
      "Iter:   5300, Train Loss:  0.021, Train Acc: 100.00%, Val Loss:   0.29, Val Acc:  93.47%, Time: 1:20:22 \n",
      "Iter:   5400, Train Loss:  0.039, Train Acc:  98.44%, Val Loss:   0.29, Val Acc:  92.94%, Time: 1:21:50 \n",
      "No optimization for a long time, auto-stopping...\n"
     ]
    }
   ],
   "source": [
    "train_scores,test_scores = train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在验证集上的最佳效果为93.79%，经过了9轮迭代停止。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print(test_scores)\n",
    "fig=plt.figure()\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "ax.plot(range(7 * config.num_epochs), train_scores, label=\"accuracy \", marker='+')\n",
    "ax.set_title(\" accuracy \")\n",
    "ax.set_xlabel(r\"r\")\n",
    "ax.set_ylabel(\"score\")\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.legend(loc=\"best\", framealpha=0.5)\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "ax.plot(range(7 * config.num_epochs), test_scores, label=\" loss \", marker='o')\n",
    "ax.set_title(\" loss \")\n",
    "ax.set_xlabel(r\"r\")\n",
    "ax.set_ylabel(\"score\")\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.legend(loc=\"best\", framealpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test data...\n",
      "INFO:tensorflow:Restoring parameters from /home/wu/文档/modeling/Scope_data/CNN_RNN/checkpoints/textcnn/best_validation\n",
      "Testing...\n",
      "Test Loss:   0.26, Test Acc:  93.42%\n",
      "Precision, Recall and F1-Score...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         服装       0.97      0.98      0.98      5265\n",
      "         工业       0.81      0.89      0.85      1209\n",
      "         农业       0.83      0.70      0.76       211\n",
      "         运输       0.94      0.86      0.90       131\n",
      "         信息       0.92      0.88      0.90      1312\n",
      "         食品       0.90      0.74      0.81       231\n",
      "         建筑       0.90      0.82      0.86       235\n",
      "         金融       0.99      0.96      0.98       236\n",
      "\n",
      "avg / total       0.93      0.93      0.93      8830\n",
      "\n",
      "Confusion Matrix...\n",
      "[[5173   61   10    3   16    1    1    0]\n",
      " [  75 1074    3    1   47    6    3    0]\n",
      " [  15   26  148    1   12    3    6    0]\n",
      " [   5    6    0  113    3    1    2    1]\n",
      " [  47   94    4    1 1150    9    7    0]\n",
      " [   3   41    5    0    9  172    1    0]\n",
      " [   7   17    5    1   12    0  192    1]\n",
      " [   0    3    4    0    1    0    1  227]]\n",
      "Time usage: 0:00:30\n"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在测试集上的准确率为93.42%\n",
    "\n",
    "从混淆矩阵也可以看出分类效果相对较好。\n",
    "\n",
    "分类效果较弱的两类为农业和食品，其他分类效果较好。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
